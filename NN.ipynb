{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthapratimkalita/ELOBSE/blob/master/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wbP6XiboZJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ed83bc-c6c5-42e3-d22a-ce12da9d53b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np #importing the numpy array as np\n",
        "\n",
        "x = np.random.randint(0, 2, size=6) #generating a random array 'x' where lowest element can be one and highest could be 2-1.\n",
        "\n",
        "t = [] #intializing an array t\n",
        "\n",
        "for i in range(len(x)):\n",
        "  m = x[i]**3 - x[i]**2\n",
        "  t.append(m)\n",
        "\n",
        "t = np.array(t)\n",
        "\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wTshi_kS0sA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layers:\n",
        "\n",
        "  def __init__(self, n_units, input_units):\n",
        "\n",
        "    self.n_units = n_units\n",
        "    self.input_units = input_units\n",
        "    \n",
        "\n",
        "    self.weights = np.random.randint(1,4, size=(self.input_units, self.n_units))\n",
        "    self.biases = np.zeros(shape=(1, self.n_units), dtype= 'i')\n",
        "\n",
        "\n",
        "  def Relu(self,m):\n",
        "\n",
        "    shape = m.shape\n",
        "    comp = np.zeros(shape=(1, shape[1]), dtype= 'i')\n",
        "    return np.maximum(m,comp)\n",
        "\n",
        "  def forward_step(self, input_values):\n",
        "\n",
        "    self.input_values = input_values\n",
        "    self.input_values = np.asarray(self.input_values).reshape(1,self.input_units)\n",
        "\n",
        "    self.sum = np.dot(self.input_values, self.weights) \n",
        "\n",
        " \n",
        "    self.sum = self.sum + self.biases\n",
        "\n",
        "    self.output = self.Relu(self.sum)\n",
        "    return self.output\n",
        "\n",
        "\n",
        "  def derivative_out_wrt_act(self, out1):\n",
        "\n",
        "      shape = out1.shape\n",
        "\n",
        "      for i in range(shape[1]):\n",
        "          if out1[0][i] <= 0:\n",
        "              out1[0][i] = 0\n",
        "          else:\n",
        "              out1[0][i] = 1\n",
        "\n",
        "      return out1\n",
        "\n",
        "\n",
        "  def loss(self, ta1):\n",
        "\n",
        "    ta1 = np.array(ta1).reshape(1, len(ta1))\n",
        "\n",
        "    #print(ta1)\n",
        "    #print(self.output)\n",
        "    derv_loss_wrt_out = self.output - ta1\n",
        "    return derv_loss_wrt_out\n",
        "\n",
        "  def hidden_layer_loss(self, loss1, weights):\n",
        "\n",
        "    hid_loss = np.dot( loss1, np.transpose(weights))\n",
        "    return hid_loss\n",
        "\n",
        "\n",
        "  def backward_step(self, lear_rate, loss, prev):\n",
        "      \n",
        "\n",
        "      #print(\"@@@@\")\n",
        "\n",
        "      derv_loss_wrt_out = loss\n",
        "      #print(derv_loss_wrt_out)\n",
        "      derv_out_wrt_act  = self.derivative_out_wrt_act(self.sum)\n",
        "      #print(derv_out_wrt_act)\n",
        "      derv_input_wrt_wei = self.input_values\n",
        "      #print(derv_input_wrt_wei)\n",
        "\n",
        "      right = derv_loss_wrt_out * derv_out_wrt_act  * prev\n",
        "\n",
        "      loss_wrt_wei = np.dot(np.transpose(derv_input_wrt_wei),  right)\n",
        "      \n",
        "      self.weights = self.weights - lear_rate * loss_wrt_wei\n",
        "\n",
        "      return [right, self.weights]\n",
        "\n",
        "    \n",
        "      #return self.output\n",
        "\n",
        "\n",
        "# Multi layer perceptron class with two layers\n",
        "\n",
        "class MLP(Layers):\n",
        "\n",
        "  Lar_h = Layers(10,1)\n",
        "  Lar_out = Layers(1,10) \n",
        "\n",
        "\n",
        "#loss calculation of the network\n",
        "  def end_loss(y, t):\n",
        "      sq = (y-t)*(y-t)\n",
        "      los = 0.5 * sq\n",
        "\n",
        "      return los\n",
        "\n",
        "  for i in range(10):\n",
        "\n",
        "      for j in range(6):\n",
        "\n",
        "# Implementing feedforward propagation on hidden layer\n",
        "        out = Lar_h.forward_step([x[j]])\n",
        "\n",
        "# Implementing feed forward propagation on output layer\n",
        "        out1 = Lar_out.forward_step(out[0])\n",
        "\n",
        "\n",
        "\n",
        "# Backpropagation phase\n",
        "\n",
        "        # Calculating the error at output nodes\n",
        "        loss = Lar_out.loss([t[j]])\n",
        "\n",
        "        back_derv_1 = Lar_out.backward_step(0.5, loss, 1)\n",
        "\n",
        "        \n",
        "        #Calculating the error at the hidden nodes\n",
        "        loss = Lar_h.hidden_layer_loss(loss, back_derv_1[1])\n",
        "\n",
        "        back_derv_2 = Lar_h.backward_step(0.5, loss, back_derv_1[0] )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Layers):\n",
        "\n",
        "  Lar_h = Layers(10,1)\n",
        "  Lar_out = Layers(1,10)\n",
        "  out = Lar_h.forward_step([x[0]])\n",
        "  out1 = Lar_out.forward_step(out[0])\n",
        "\n",
        "  print(out1)\n",
        "\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMxgTaS8Fwhg",
        "outputId": "2b6ca34a-09c1-4726-a023-f783b59214ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[33]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Layers):\n",
        "\n",
        "  Lar_h = Layers(10,1)\n",
        "  Lar_out = Layers(1,10) \n",
        "\n",
        "  def end_loss(y, t):\n",
        "\n",
        "      los\n",
        "      sq = (y-t)*(y-t)\n",
        "      los = 0.5 * sq\n",
        "\n",
        "      return los\n",
        "\n",
        "\n",
        "  if i is 0:\n",
        "            target = np.array(np.zeros(6)).reshape(1, 6)\n",
        "            end_los = end_loss(out1[0,0], t[j])\n",
        "            target[0][j] = end_los\n",
        "\n",
        "        else:\n",
        "            target1 = np.array(np.zeros(6)).reshape(1, 6)\n",
        "            end_los = end_loss(out1[0,0], t[j])\n",
        "            target1[0][j] = end_los\n",
        "\n",
        "  for i in range(2):\n",
        "\n",
        "      for j in range(1):\n",
        "\n",
        "        out = Lar_h.forward_step([x[j]])\n",
        "        out1 = Lar_out.forward_step(out[0])\n",
        "\n",
        "        comp_tar = np.array([]) \n",
        "\n",
        "        \n",
        "\n",
        "        comp_tar = np.append(target, target1, axis = 0)\n",
        "            \n",
        "\n",
        "        loss = Lar_out.loss([t[j]])\n",
        "\n",
        "        back_derv_1 = Lar_out.backward_step(0.5, loss, 1)\n",
        "\n",
        "        loss = Lar_h.hidden_layer_loss(loss, back_derv_1[1])\n",
        "\n",
        "        back_derv_2 = Lar_h.backward_step(0.5, loss, back_derv_1[0] )\n",
        "\n",
        "\n",
        "  print(\"loss after each epoch\")\n",
        "  print(comp_tar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "nO4a6QTTF3y-",
        "outputId": "6ed25cf2-d8e1-4b6f-98e8-92768c4ccf6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-9d3ba28baa77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mLar_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mLar_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-9d3ba28baa77>\u001b[0m in \u001b[0;36mMLP\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtarget1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_los\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mcomp_tar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "target = np.zeros(6)\n",
        "target = np.array(target).reshape(1, len(target))\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqQT806iMhzQ",
        "outputId": "48f98c1e-a7c9-4289-8ee3-a2f7c3df2de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "plEt6ONpod80",
        "outputId": "b6807547-2c07-4498-8c3d-b574e207e4c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[3 1 3 2 2 1 3 3 2 2]]\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [3]\n",
            " [1]\n",
            " [3]\n",
            " [3]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "[[3 1 3 2 2 1 3 3 2 2]]\n",
            "[[34]]\n",
            "derv_loss_wrt_wei1\n",
            "[[34]]\n",
            "derv_loss_wrt_wei\n",
            "[[102 102 102 102 102 102 102 102 102 102]\n",
            " [ 34  34  34  34  34  34  34  34  34  34]\n",
            " [102 102 102 102 102 102 102 102 102 102]\n",
            " [ 68  68  68  68  68  68  68  68  68  68]\n",
            " [ 68  68  68  68  68  68  68  68  68  68]\n",
            " [ 34  34  34  34  34  34  34  34  34  34]\n",
            " [102 102 102 102 102 102 102 102 102 102]\n",
            " [102 102 102 102 102 102 102 102 102 102]\n",
            " [ 68  68  68  68  68  68  68  68  68  68]\n",
            " [ 68  68  68  68  68  68  68  68  68  68]]\n",
            "@@@@@@@@@@@@@\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [3]\n",
            " [1]\n",
            " [3]\n",
            " [3]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "[[-50. -50. -50. -50. -50. -50. -50. -50. -50. -50.]\n",
            " [-16. -16. -16. -16. -16. -16. -16. -16. -16. -16.]\n",
            " [-50. -50. -50. -50. -50. -50. -50. -50. -50. -50.]\n",
            " [-31. -31. -31. -31. -31. -31. -31. -31. -31. -31.]\n",
            " [-33. -33. -33. -33. -33. -33. -33. -33. -33. -33.]\n",
            " [-14. -14. -14. -14. -14. -14. -14. -14. -14. -14.]\n",
            " [-48. -48. -48. -48. -48. -48. -48. -48. -48. -48.]\n",
            " [-50. -50. -50. -50. -50. -50. -50. -50. -50. -50.]\n",
            " [-33. -33. -33. -33. -33. -33. -33. -33. -33. -33.]\n",
            " [-33. -33. -33. -33. -33. -33. -33. -33. -33. -33.]]\n",
            "[[1 1 1 1 1 1 1 1 1 1]]\n",
            "[[510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]]\n",
            "derv_loss_wrt_wei1\n",
            "[[510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]]\n",
            "derv_loss_wrt_wei\n",
            "[[510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]\n",
            " [510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594. 510594.\n",
            "  510594.]]\n",
            "@@@@@@@@@@@@@\n",
            "[[86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650.]\n",
            " [ 9232.  9232.  9232.  9232.  9232.  9232.  9232.  9232.  9232.  9232.]\n",
            " [86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650.]\n",
            " [35805. 35805. 35805. 35805. 35805. 35805. 35805. 35805. 35805. 35805.]\n",
            " [38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115.]\n",
            " [ 8078.  8078.  8078.  8078.  8078.  8078.  8078.  8078.  8078.  8078.]\n",
            " [83184. 83184. 83184. 83184. 83184. 83184. 83184. 83184. 83184. 83184.]\n",
            " [86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650. 86650.]\n",
            " [38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115.]\n",
            " [38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115. 38115.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layers:\n",
        "\n",
        "  def __init__(self, n_units, input_units):\n",
        "\n",
        "    self.n_units = n_units\n",
        "    self.input_units = input_units\n",
        "    \n",
        "\n",
        "    self.weights = np.random.randint(1,4, size=(self.input_units, self.n_units))\n",
        "    self.biases = np.zeros(shape=(1,self.n_units), dtype= 'i')\n",
        "    #print(self.biases)\n",
        "\n",
        "\n",
        "  def Relu(self,x,y):\n",
        "    return np.maximum(x,y)\n",
        "\n",
        "  def forward_step(self, input_values):\n",
        "\n",
        "    self.input_values = input_values\n",
        "    self.input_values = np.array(self.input_values)\n",
        "\n",
        "\n",
        "    #print(self.weights)\n",
        "    self.sum = np.dot(self.input_values, self.weights) \n",
        "\n",
        "    #print(self.sum)\n",
        "\n",
        "    self.sum = self.sum + self.biases\n",
        "\n",
        "    #print(\"****\")\n",
        "\n",
        "    #print(self.sum)\n",
        "\n",
        "\n",
        "    comp_relu = np.empty(shape = (1,len(self.sum)), dtype ='i')\n",
        "    self.output = self.Relu(self.sum, comp_relu)\n",
        "\n",
        "    #print(self.output)\n",
        "    return self.output\n",
        "\n",
        "  def derivative_out_wrt_act(self, out):\n",
        "\n",
        "      self.output = out\n",
        "\n",
        "      shape = self.output.shape\n",
        "\n",
        "      for i in range(shape[1]):\n",
        "          if self.output[0][i] <= 0:\n",
        "            self.output[0][i] = 0\n",
        "          else:\n",
        "            self.output[0][i] = 1\n",
        "\n",
        "      return self.output\n",
        "\n",
        "  def backward_step(self, target, lear_rate):\n",
        "    \n",
        "    self.target = target\n",
        "    self.learn_rate = lear_rate\n",
        "\n",
        "\n",
        "    dim = self.output.shape\n",
        "    unit_matrix = np.ones(dim, dtype=\"i\")\n",
        "\n",
        "    #print(self.weights)\n",
        "    #print(\"******\")\n",
        "\n",
        "    derv_loss_wrt_out = self.output - self.target \n",
        "    derv_out_wrt_actv  = self.derivative_out_wrt_act(self.output)\n",
        "\n",
        "    derv_inp_wrt_wei = np.array([self.input_values])\n",
        "    #print(derv_inp_wrt_wei.shape)\n",
        "\n",
        "    for i in range(0, 99):\n",
        "      derv_inp_wrt_wei = np.append(derv_inp_wrt_wei, [self.input_values], axis=0)\n",
        "\n",
        "    derv_inp_wrt_wei = np.transpose(derv_inp_wrt_wei)\n",
        "\n",
        "\n",
        "    derv_loss_wrt_wei = derv_loss_wrt_out * derv_out_wrt_actv\n",
        "\n",
        "    print(\"\")\n",
        "    print(derv_inp_wrt_wei)\n",
        "\n",
        "    derv_loss_wrt_wei = derv_inp_wrt_wei * derv_loss_wrt_wei\n",
        "\n",
        "    #print(\"derv_out_wrt_actv\")\n",
        "    #print(derv_out_wrt_actv)\n",
        "    #print(\"derv_loss_wrt_out\")\n",
        "    #print(derv_loss_wrt_out)\n",
        "    #print(\"derv_inp_wrt_wei\")\n",
        "    #print(derv_inp_wrt_wei)\n",
        "    print(\"derv_loss_wrt_wei\")\n",
        "    print(derv_loss_wrt_wei)\n",
        "\n",
        "    self.weights = self.weights - lear_rate * derv_loss_wrt_wei\n",
        "\n",
        "    print(\"update weights\")\n",
        "    print(self.weights)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "obj = Layers(100,100)\n",
        "\n",
        "out = obj.forward_step(x)\n",
        "wei = obj.backward_step(t, 0.5)\n",
        "\n",
        "print(\"*****\")\n",
        "\n",
        "out = obj.forward_step(x)\n",
        "wei = obj.backward_step(t, 0.5)\n",
        "\n",
        "print(\"*****\")\n",
        "\n",
        "out = obj.forward_step(x)\n",
        "wei = obj.backward_step(t, 0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqgnn6u9eRZW"
      },
      "outputs": [],
      "source": [
        "derv_inp_wrt_wei = np.transpose(derv_inp_wrt_wei)\n",
        "\n",
        "\n",
        "    derv_loss_wrt_wei = derv_loss_wrt_out * derv_out_wrt_actv\n",
        "\n",
        "    print(derv_inp_wrt_wei.shape)\n",
        "\n",
        "    derv_loss_wrt_wei = np.dot(derv_inp_wrt_wei, derv_loss_wrt_wei)\n",
        "\n",
        "    print(\"derv_out_wrt_actv\")\n",
        "    print(derv_out_wrt_actv)\n",
        "    print(\"derv_loss_wrt_out\")\n",
        "    print(derv_loss_wrt_out)\n",
        "    print(\"derv_inp_wrt_wei\")\n",
        "    print(derv_inp_wrt_wei)\n",
        "    print(\"derv_loss_wrt_wei\")\n",
        "    print(derv_loss_wrt_wei)\n",
        "\n",
        "    self.weights = self.weights - lear_rate * derv_loss_wrt_wei\n",
        "\n",
        "  \n",
        "    print(self.weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTyJyGEEhrPq",
        "outputId": "cda66bf7-f683-4a07-cb72-7134da4077d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 2 3]\n",
            " [2 3 5]]\n",
            "[[ 2  4  6]\n",
            " [ 4  6 10]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.array([[1,2,3]])\n",
        "x = np.append(x, [[2,3,5]], axis =0)\n",
        "print(x)\n",
        "x = x * [2,2,2]\n",
        "print(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjc4oUKVFMUSjifJZ5FdwT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}